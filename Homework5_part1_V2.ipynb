{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Urvi-M/BE559/blob/main/Homework5_part1_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh2ENMCBHlev"
      },
      "source": [
        "# Homework 5.1 (31 points)\n",
        "\n",
        "**Learning objectives:**\n",
        "\n",
        "In this assignment, we will become familiar with PyTorch, one of the most popular libraries for machine learning.  We will explore:\n",
        "\n",
        "*   PyTorch syntax and usage\n",
        "*   Solving the XOR problem with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu8GMkSHHley"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0\n",
        "\n",
        "This assignment will provide an introduction to PyTorch. PyTorch is an open source machine learning framework that allows you to write your own neural networks and optimize them efficiently. We choose to use PyTorch because it is well established, has a huge developer community (originally developed by Facebook), is very flexible and especially used in research.\n",
        "\n",
        "Meanwhile, TensorFlow (developed by Google) is usually known for being a production-grade deep learning library.\n",
        "\n",
        "---\n",
        "\n",
        "First, load the same libraries that we used in previous homeworks.\n",
        "\n",
        "*   `numpy` (as `np`)\n",
        "*   `matplotlib.pyplot` (as `plt`)\n",
        "\n",
        "Now, import **Pytorch**.  The package is called `torch`.\n",
        "\n",
        "*   `import torch`\n",
        "\n",
        "As always, now is also a good time to set the default font size to 16.\n",
        "\n",
        "Finally, let's set the random seed for `torch` so that our code's output is the same every time we run it.\n",
        "\n",
        "*   `torch.manual_seed(1)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8548yZX4Hlez"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "torch.manual_seed(1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFWZv5dRHle4"
      },
      "source": [
        "---\n",
        "\n",
        "### Part 1  (1 point)\n",
        "\n",
        "**Tensors** are the PyTorch equivalent of Numpy arrays, but also have support for GPU acceleration (more on that later). The name \"tensor\" really just refers to higher-dimensional matrices (3-D, 4-D, etc.).\n",
        "\n",
        "Most common functions you know from numpy can be used on tensors as well. Since numpy arrays are so similar to tensors, we can convert most tensors to numpy arrays (and back).\n",
        "\n",
        "Let's first start by looking at different ways of creating a tensor. There are **many** possible options.\n",
        "\n",
        "*   Create a 3-D tensor called `xtensor` of dimensions 2x3x4 using `torch.Tensor`.  This allocates memory for the desired tensor, but reuses any values that might already be in memory.  Print `xtensor`.\n",
        "*   Create a tensor that is the same size but contains zeros using `torch.zeros`.  Call it `xzeros` and print it.\n",
        "*   Create and print similar tensors using `torch.ones` (called `xones`), `torch.rand` (called `xrand`), and `torch.randn` (called `xrandn`).\n",
        "*   Create and print a 1-D tensor using `torch.arange` (called `xarange`) containing the integers 1 through 10.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kQ4tinZYHle5",
        "outputId": "db7e508d-8ca9-4a87-d9c3-7de1ab89b7b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.1519e-12,  4.3856e-41, -6.5188e-28,  3.3219e-41],\n",
            "         [ 1.4013e-45,  0.0000e+00, -4.8409e-28,  3.3219e-41],\n",
            "         [-6.6626e-28,  3.3219e-41,  0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[-6.6631e-28,  3.3219e-41, -9.4505e-29,  3.3219e-41],\n",
            "         [ 1.0899e-08,  4.3855e-41,  0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "tensor([[[0.2197, 0.4177, 0.4903, 0.5730],\n",
            "         [0.1205, 0.1452, 0.7720, 0.3828],\n",
            "         [0.7442, 0.5285, 0.6642, 0.6099]],\n",
            "\n",
            "        [[0.6818, 0.7479, 0.0369, 0.7517],\n",
            "         [0.1484, 0.1227, 0.5304, 0.4148],\n",
            "         [0.7937, 0.2104, 0.0555, 0.8639]]])\n",
            "tensor([[[-0.5047, -1.4746, -0.3416, -0.3003],\n",
            "         [ 1.3075, -1.1628,  0.1196, -0.1631],\n",
            "         [ 0.8738, -0.5603,  1.2858,  0.8168]],\n",
            "\n",
            "        [[ 0.2053,  0.3051,  0.5357, -0.4312],\n",
            "         [ 2.5581, -0.2334, -0.0135,  1.8606],\n",
            "         [-1.9804,  1.7986,  0.1018,  0.3400]]])\n",
            "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
          ]
        }
      ],
      "source": [
        "xtensor = torch.Tensor(2, 3, 4)\n",
        "print(xtensor)\n",
        "xzeros= torch.zeros(2,3,4)\n",
        "print(xzeros)\n",
        "xones= torch.ones(2,3,4)\n",
        "print(xones)\n",
        "xrand = torch.rand(2,3,4)\n",
        "print(xrand)\n",
        "xrandn=torch.randn(2,3,4)\n",
        "print(xrandn)\n",
        "xarange = torch.arange(1,11,1)\n",
        "print(xarange)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-al5ZdKHle-"
      },
      "source": [
        "---\n",
        "## Part 2 (1 point)\n",
        "\n",
        "Tensors can be converted to numpy arrays, and numpy arrays back to tensors. To transform a numpy array into a tensor, we can use the function `torch.from_numpy`.\n",
        "*   Create a 2x2 numpy array called `np_arr` with the values 1,2 in the first row and 3,4 in the second row.\n",
        "*   Convert `np_arr` to a PyTorch tensor called `pt_tensor`.  \n",
        "*   Print both `np_arr` and `pt_tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Q0M4L-VOHle-",
        "outputId": "91a6f6cb-6f61-4442-8bef-ed8ec7ac5801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy array = [[1 2]\n",
            " [3 4]]\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ],
      "source": [
        "np_arr =np.array([[1,2],\n",
        "          [3,4]])\n",
        "pt_tensor= torch.from_numpy(np_arr)\n",
        "print('Numpy array =', np_arr)\n",
        "print(pt_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZExk9WVHle_"
      },
      "source": [
        "To transform a PyTorch tensor back to a numpy array, we can use the function `.numpy()` on tensors:\n",
        "\n",
        "*   Create a 1-D tensor called `pt_tensor` containing the integers 0 through 3 and convert it to a numpy array called `np_arr`.\n",
        "*   Print them both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3ZaBzgO0Hle_",
        "outputId": "3eca5500-9cc6-464e-e04d-c06e92600a7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch tensor: tensor([0, 1, 2, 3])\n",
            "Numpy tensor: [0 1 2 3]\n"
          ]
        }
      ],
      "source": [
        "pt_tensor= torch.arange(0,4)\n",
        "print('PyTorch tensor:',pt_tensor)\n",
        "pt_tensor= torch.arange(0,4).numpy()\n",
        "print('Numpy tensor:',pt_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOBmNvhoHle_"
      },
      "source": [
        "The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU (more on GPU support in a later section). In case you have a tensor on GPU, you need to call `.cpu()` on the tensor beforehand. Hence, you get a line like `np_arr = tensor.cpu().numpy()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Z3IAHPHle_"
      },
      "source": [
        "---\n",
        "## Part 3 (1 point)\n",
        "\n",
        "Most operations that exist in numpy, also exist in PyTorch. A full list of operations can be found in the [PyTorch documentation](https://pytorch.org/docs/stable/tensors.html#), but we will review the most important ones here.\n",
        "\n",
        "The simplest operation is to add two tensors:\n",
        "*   Create two 2x3 tensors of random numbers, `x1` and `x2`\n",
        "*   Create a new variable, `y`, equal to `x1+x2`\n",
        "*   Print `x1`, `x2`, and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2oHbuhUbHle_",
        "outputId": "715f9f62-a981-4cdd-8454-2a02f5ce5dce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 tensor([[0.2709, 0.4418, 0.1935],\n",
            "        [0.6829, 0.6547, 0.3868]])\n",
            "X2 tensor([[0.6922, 0.6616, 0.8053],\n",
            "        [0.8367, 0.3307, 0.9885]])\n",
            "Y tensor([[0.9631, 1.1034, 0.9988],\n",
            "        [1.5196, 0.9854, 1.3753]])\n"
          ]
        }
      ],
      "source": [
        "x1 = torch.rand(2, 3)\n",
        "x2 = torch.rand(2, 3)\n",
        "y = x1 + x2\n",
        "\n",
        "print(\"X1\", x1)\n",
        "print(\"X2\", x2)\n",
        "print(\"Y\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3mCgOfRHlfA"
      },
      "source": [
        "Calling `x1 + x2` creates a new tensor containing the sum of the two inputs. However, we can also use in-place operations that are applied directly on the memory of a tensor.\n",
        "*   Use the `.add_` method to modify the value of `x2` so that it is now equal to `x1+x2`.\n",
        "*   Print the new `x2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iJ4AMsLXHlfA",
        "outputId": "33364a78-b8b2-40e4-da4a-dacecb41d555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 (before) tensor([[0.2709, 0.4418, 0.1935],\n",
            "        [0.6829, 0.6547, 0.3868]])\n",
            "X2 (before) tensor([[0.6922, 0.6616, 0.8053],\n",
            "        [0.8367, 0.3307, 0.9885]])\n",
            "X1 (after) tensor([[0.2709, 0.4418, 0.1935],\n",
            "        [0.6829, 0.6547, 0.3868]])\n",
            "X2 (after) tensor([[0.9631, 1.1034, 0.9988],\n",
            "        [1.5196, 0.9854, 1.3753]])\n"
          ]
        }
      ],
      "source": [
        "print('X1 (before)',x1)\n",
        "print('X2 (before)',x2)\n",
        "print('X1 (after)',x1)\n",
        "x2.add_(x1)\n",
        "print('X2 (after)',x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJNBKjkVHlfB"
      },
      "source": [
        "In-place operations are usually marked with a underscore postfix (e.g. \"add_\" instead of \"add\").\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 4 (1 point)\n",
        "\n",
        "Another common operation is to change the shape of a tensor. A tensor of size (2,3) can be re-organized to any other shape with the same number of elements . In PyTorch, this operation is called `view`.  \n",
        "\n",
        "*  Use `view` to transform the elements of `x1` to a new tensor of size 6 called `x3`.\n",
        "*  Use `view` to transform the elements of `x1` to a new tensor of size 3x2 called `x4`.\n",
        "*   Print `x3` and `x4`."
      ],
      "metadata": {
        "id": "ACWGzd0pwm4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "I1B73jRtHlfB",
        "outputId": "e137ea6b-411f-4d9d-b9ea-fb9deb737d26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2709, 0.4418, 0.1935, 0.6829, 0.6547, 0.3868]])\n"
          ]
        }
      ],
      "source": [
        "x3 = x1.view(1,6)\n",
        "print(x3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9BISJ0W4HlfB",
        "outputId": "295d5fb9-f2f7-47b6-d917-442825a22a8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2709, 0.4418],\n",
            "        [0.1935, 0.6829],\n",
            "        [0.6547, 0.3868]])\n"
          ]
        }
      ],
      "source": [
        "x4 = x1.view(3,2)\n",
        "print(x4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Izma2CHlfD"
      },
      "source": [
        "---\n",
        "## Part 5 (1 point)\n",
        "\n",
        "Other commonly used operations include matrix multiplications, which are essential for neural networks. Quite often, we have an input vector $\\mathbf{x}$, which is transformed using a learned weight matrix $\\mathbf{W}$. There are multiple ways and functions to perform matrix multiplication, some of which we list below:\n",
        "\n",
        "* `torch.matmul`: Performs the matrix product over two tensors. If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product. For higher dimensional inputs, the function supports broadcasting (for details see the [documentation](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)). Can also be written as `a @ b`, similar to numpy.\n",
        "\n",
        "* `torch.bmm`: Performs the matrix product and supports a batch dimension (effectively the number of training examples in the array). If the first tensor $T$ is of shape ($b\\times n\\times m$), and the second tensor $R$ ($b\\times m\\times p$), the output $O$ is of shape ($b\\times n\\times p$), and has been calculated by performing $b$ matrix multiplications of the $n\\times m$ and $m\\times p$ submatrices of $T$ and $R$.\n",
        "\n",
        "*   Create a 2x3 tensor called **`x`** of the digits 0 through 5 using the `arange` and `view` methods.\n",
        "*   Create a 3x3 tensor called **`W`** containing the digits 0 through 8 in the same way.\n",
        "*   Use the `.matmul` method to multiply **`x`** and **`W`**.\n",
        "*   Print the output of this operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sGp1HeZEHlfD",
        "outputId": "3cb8719c-f27c-4903-a2d8-cff38d9daaed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X = tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n"
          ]
        }
      ],
      "source": [
        "x= torch.arange(0,6).view(2,3)\n",
        "print('X =',x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uYLDrV7aHlfD",
        "outputId": "1ceb3f9e-fc14-4a49-f97e-c527773f4056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n"
          ]
        }
      ],
      "source": [
        "W= torch.arange(0,9).view(3,3)\n",
        "print('W',W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3BsFwIK1HlfD",
        "outputId": "105740cf-0306-4b04-9184-47d0a83fdcba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h tensor([[15, 18, 21],\n",
            "        [42, 54, 66]])\n"
          ]
        }
      ],
      "source": [
        "h=torch.matmul(x,W)\n",
        "print('h',h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLV_p3noHlfG"
      },
      "source": [
        "---\n",
        "\n",
        "### Gradients\n",
        "\n",
        "One of the main reasons for using PyTorch in Deep Learning projects is that we can automatically get **gradients** of functions that we define. We will mainly use PyTorch for implementing neural networks, and they are just fancy functions. If we use weight matrices in our function that we want to learn, then those are called the **parameters** or simply the **weights**.\n",
        "\n",
        "If our neural network had a single scalar input, we would talk about taking the derivative, but you will see that quite often we will have multiple input variables; in that case we talk about **gradients**. It's a more general term.\n",
        "\n",
        "Given an input $\\mathbf{x}$, we define our function by manipulating that input, usually by matrix-multiplications with weight matrices and additions with so-called bias vectors. As we manipulate our input, we are automatically creating a **computational graph**. This graph shows how to arrive at our output from our input.\n",
        "PyTorch is a **define-by-run** framework; this means that we can just do our manipulations, and PyTorch will keep track of that graph for us. Thus, we create a dynamic computation graph along the way.\n",
        "\n",
        "So, to recap: the only thing we have to do is to compute the **output**, and then we can ask PyTorch to automatically get the **gradients**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 6 (1 point)\n",
        "\n",
        "\n",
        "The first thing we have to do is to specify which tensors require gradients. By default, when we create a tensor, it does not require gradients.\n",
        "\n",
        "*  Create a 3-element tensor called `x` in which all elements are equal to 1.\n",
        "*  Print the output of calling `x.requires_grad`."
      ],
      "metadata": {
        "id": "n-OAMXsg2yUe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "imuTPHv0HlfG",
        "outputId": "f4d8cb18-dcc4-45b3-d395-ea53e395a66f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "x= torch.ones(3)\n",
        "print(x.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ3u19jXHlfH"
      },
      "source": [
        "We can change this for an existing tensor using the method `requires_grad_(True)` (underscore indicating that this is a in-place operation). Alternatively, when creating a tensor, you can pass the argument `requires_grad=True` to most initializers we have seen above.\n",
        "\n",
        "*  Specify that `x` requires gradients and print the output of `x.requires_grad` again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "66bGDQxWHlfI",
        "outputId": "d1fd2f69-5202-4d75-8d96-3769d398e9ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "x.requires_grad=True\n",
        "print(x.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7qnoH40HlfI"
      },
      "source": [
        "---\n",
        "## Part 7 (1 point)\n",
        "\n",
        "In order to get familiar with the concept of a computation graph, we will create one for the following function:\n",
        "\n",
        "$$y = \\frac{1}{|x|}\\sum_i \\left[(x_i + 2)^2 + 3\\right]$$\n",
        "\n",
        "You could imagine that $x$ are our parameters, and we want to optimize (either maximize or minimize) the output $y$. For this, we want to obtain the gradients $\\partial y / \\partial \\mathbf{x}$. For our example, we'll use $\\mathbf{x}=[0,1,2]$ as our input.\n",
        "\n",
        "*  Create $\\mathbf{x}$ as above using `arange`.  Be sure to use the option `requires_grad=True`.  Also set `dtype=torch.float32`.  Only float tensors can have gradients.\n",
        "*   Print $\\mathbf{x}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "VtOfD0lfHlfI",
        "outputId": "5a9149b3-f1b3-4331-febf-d80668605ea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([0., 1., 2.], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "X = torch.arange(3,requires_grad=True, dtype=torch.float32)\n",
        "print('X',X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQdx9HEKHlfJ"
      },
      "source": [
        "---\n",
        "## Part 8 (1 point)\n",
        "\n",
        "Now let's build the **computation graph** step by step. You can combine multiple operations in a single line, but we will separate them here to get a better understanding of how each operation is added to the computation graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-4-DYbNyHlfJ",
        "outputId": "81bc4e13-be73-42ba-c4bb-00596c1b500f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y tensor(12., grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "a = x + 2\n",
        "b = a ** 2\n",
        "c = b + 3\n",
        "y = c.mean()\n",
        "print(\"Y\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxewrjPFHlfJ"
      },
      "source": [
        "Using the statements above, we have created a computation graph that looks similar to the figure below:\n",
        "\n",
        "<center style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/pytorch_computation_graph.svg?raw=1\" width=\"200px\"></center>\n",
        "\n",
        "The visualization is an abstraction of the dependencies between inputs and outputs of the operations we have applied.\n",
        "\n",
        "Each node of the computation graph has automatically defined a function for calculating the gradients with respect to its inputs, `grad_fn`.\n",
        "\n",
        "You can see this when we printed the output tensor $y$. This is why the computation graph is usually visualized in the reverse direction (arrows point from the result to the inputs). We can perform backpropagation on the computation graph by calling the function `backward()` on the last output, which effectively calculates the gradients for each tensor that has the property `requires_grad=True`.\n",
        "\n",
        "*  Apply the `.backward()` method to `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUCL3dd9HlfJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaYH9Mh2HlfK"
      },
      "source": [
        "`x.grad` will now contain the gradient $\\partial y/ \\partial \\mathcal{x}$, and this gradient indicates how a change in $\\mathbf{x}$ will affect output $y$ given the current input $\\mathbf{x}=[0,1,2]$.\n",
        "*   Print `x.grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm9PYwK3HlfK",
        "outputId": "d9f1a4f2-6aef-4465-fc07-ae833452c43a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.3333, 2.0000, 2.6667])\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPZftKamHlfK"
      },
      "source": [
        "---\n",
        "\n",
        "We can also verify these gradients by hand. We will calculate the gradients using the chain rule, in the same way as PyTorch did it:\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial c_i}\\frac{\\partial c_i}{\\partial b_i}\\frac{\\partial b_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i}$$\n",
        "\n",
        "The partial derivatives are:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial a_i}{\\partial x_i} = 1,\\hspace{1cm}\n",
        "\\frac{\\partial b_i}{\\partial a_i} = 2\\cdot a_i\\hspace{1cm}\n",
        "\\frac{\\partial c_i}{\\partial b_i} = 1\\hspace{1cm}\n",
        "\\frac{\\partial y}{\\partial c_i} = \\frac{1}{3}\n",
        "$$\n",
        "\n",
        "Hence, with the input being $\\mathbf{x}=[0,1,2]$, our gradients are $\\partial y/\\partial \\mathbf{x}=[4/3,2,8/3]$. The previous code cell should have printed the same result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9q0UpMBHlfL"
      },
      "source": [
        "---\n",
        "## Part 9 (1 point)\n",
        "\n",
        "A crucial feature of PyTorch is **support of GPUs**. A GPU can perform many thousands of small operations in parallel, making it very well suited for performing large matrix operations in neural networks.\n",
        "\n",
        "CPUs and GPUs have both different advantages and disadvantages, which is why many computers contain both components and use them for different tasks.\n",
        "\n",
        "GPUs can accelerate the training of your network up to a factor of $100$, which is essential for large neural networks. First, let's check whether you have a GPU available.\n",
        "\n",
        "*   Use `torch.cuda.is_available()` to determine if you have a GPU available.  Print the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xScxUf6KHlfL",
        "outputId": "c3310549-2496-40f2-e6ca-c287bb0fd04c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the GPU available? False\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb0OCU5gHlfL"
      },
      "source": [
        "On Google Colab, by default you only have access to a CPU, and so initially, the output of this cell should be `False`.\n",
        "\n",
        "Set up this session so that it has access to a GPU.  Go to `Runtime-> Change runtime type` and make sure `Hardware accelerator` is set to `GPU` and `GPU class` is set to `Standard`.  Press `Save`.  This will restart your runtime so you will have to re-run your code.  Now, the output of the above cell should be `True`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##Part 10 (1 point)\n",
        "\n",
        "By default, all tensors you create are stored on the CPU. We can push a tensor to the GPU by using the function `.to(...)`, or `.cuda()`. However, it is often a good practice to define a `device` object in your code which points to the GPU if you have one, and otherwise to the CPU. Then, you can write your code with respect to this device object, and it allows you to run the same code on both a CPU-only system, and one with a GPU. Let's try it below. We can specify the device as follows.\n"
      ],
      "metadata": {
        "id": "fhcOIXVxBXtF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipKtfcZbHlfM",
        "outputId": "69764b3e-b880-4a14-d263-37fb1b4a5d3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKPWC3wvHlfM"
      },
      "source": [
        "Now let's create a tensor and push it to the device.\n",
        "*   Create a 2x3 tensor called `x` containing zeros.\n",
        "*   Use the `.to(...)` method to push `x` to your GPU.\n",
        "*   Print `x` to verify that it is on your GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHs0E3ukHlfM",
        "outputId": "3b8dd512-2753-498c-a012-3e33c257528f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtJorOxDHlfN"
      },
      "source": [
        "In case you have a GPU, you should now see the attribute `device='cuda:0'` being printed next to your tensor. The zero next to cuda indicates that this is the zero-th GPU device on your computer. PyTorch also supports multi-GPU systems, but you will only need this if you have very big networks to train."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##Part 11 (1 point)\n",
        "\n",
        "We can compare the runtime of a large matrix multiplication on the CPU with an operation on the GPU.  \n",
        "\n",
        "*   Perform a matrix multiplication of the matrix `x` with itself on the CPU, and again on the GPU.  \n",
        "*   Time both operations and print the CPU and GPU computation times for this operation.  \n",
        "\n",
        "NOTE: The first time you run on the GPU, it may not run very fast, so run this part of your code more than once.\n",
        "\n",
        "You can use the following code skeleton for this.  All you need to fill in here is the code you want to time.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HrWmetxGCXo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeQIPiAIHlfN",
        "outputId": "e6ed0383-0e92-483f-f4b8-d14eae3e86c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU time: 3.13057s\n",
            "GPU time: 0.08785s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "x = torch.randn(5000, 5000)\n",
        "\n",
        "## CPU version\n",
        "start_time = time.time()\n",
        "## FILL IN CODE HERE\n",
        "end_time = time.time()\n",
        "print(f\"CPU time: {(end_time - start_time):6.5f}s\")\n",
        "\n",
        "## GPU version\n",
        "x = x.to(device)\n",
        "# CUDA is asynchronous, so we need to use different timing functions\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "start.record()\n",
        "## FILL IN CODE HERE\n",
        "end.record()\n",
        "torch.cuda.synchronize()  # Waits for everything to finish running on the GPU\n",
        "print(f\"GPU time: {0.001 * start.elapsed_time(end):6.5f}s\")  # Milliseconds to seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaoeYylWHlfN"
      },
      "source": [
        "Depending on the size of the operation and the CPU/GPU in your system, the speedup of this operation can be >50x. As `matmul` operations are very common in neural networks, we can already see the great benefit of training a NN on a GPU.\n",
        "\n",
        "When generating random numbers, the seed between CPU and GPU is not synchronized. Hence, we need to set the seed on the GPU separately to ensure reproducible code. Note that due to different GPU architectures, running the same code on different GPUs does not guarantee the same random numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BznDnG8hHlfO"
      },
      "outputs": [],
      "source": [
        "# GPU operations have a separate seed we also want to set\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1)\n",
        "\n",
        "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
        "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U58JqZ-YHlfO"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "Now we will construct and train a NN to solve the **Continuous XOR problem**.\n",
        "\n",
        "If we want to build a neural network in PyTorch, we could specify all our parameters (weight matrices, bias vectors) using `Tensors` (with `requires_grad=True`), ask PyTorch to calculate the gradients and then adjust the parameters. But things can quickly get cumbersome if we have a lot of parameters. In PyTorch, there is a package called `torch.nn` that makes building neural networks more convenient.\n",
        "\n",
        "We will introduce the libraries and all additional parts you might need to train a neural network in PyTorch, using an example classifier on a simple yet well known example: XOR. Given two binary inputs $x_1$ and $x_2$, the label to predict is $1$ if either $x_1$ or $x_2$ is $1$ while the other is $0$, or the label is $0$ in all other cases.\n",
        "\n",
        "We will learn how to build a small neural network that can learn this function.\n",
        "To make it a little bit more interesting, we move the XOR into continuous space and introduce some gaussian noise on the binary inputs. Our desired separation of an XOR dataset could look as follows:\n",
        "\n",
        "<center style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/continuous_xor.svg?raw=1\" width=\"350px\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MuuSnjUHlfO"
      },
      "source": [
        "##Part 12 (1 point)\n",
        "The package `torch.nn` defines a series of useful classes like linear networks layers, activation functions, loss functions etc.  \n",
        "\n",
        "\n",
        "*   Import `torch.nn` as `nn`.\n",
        "\n",
        "*   Also import `torch.nn.functional`. It contains functions that are used in network layers. Call it `F`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZFYCsvcHlfO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV6GtPUrHlfP"
      },
      "source": [
        "---\n",
        "\n",
        "In PyTorch, a neural network is built up out of modules. Modules can contain other modules, and a neural network is considered to be a module itself as well. The basic template of a module is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1hjkxGRHlfP"
      },
      "outputs": [],
      "source": [
        "class MyModule(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Some init for my module\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Function for performing the calculation of the module.\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0HsLPZWHlfQ"
      },
      "source": [
        "The forward function is where the computation of the module is taken place, and is executed when you call the module (`nn = MyModule(); nn(x)`). In the init function, we usually create the parameters of the module, using `nn.Parameter`, or defining other modules that are used in the forward function. The backward calculation is done automatically, but could be overwritten as well if wanted.\n",
        "\n",
        "This code defines a **class**.  For those of you who have not encountered classes, they are objects that can include multiple functions and variables.  We have not (and will not) go too far into class construction, but you have been using them all semester - they are 'under the hood' of the libraries we import.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We can now make use of the pre-defined modules in the `torch.nn` package, and define our **own small neural network**. We will use a minimal network with an input layer, one hidden layer using the `tanh` activation function, and an output layer. In other words, our networks should look something like this:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/small_neural_network.svg?raw=1\" width=\"300px\"></center>\n",
        "\n",
        "The input neurons are shown in blue, which represent the coordinates $x_1$ and $x_2$ of a data point. The hidden neurons including a tanh activation are shown in white, and the output neuron is in red.\n",
        "In PyTorch, we can define this as follows. Familiarize yourself with this code."
      ],
      "metadata": {
        "id": "ajoXdTfrkmpJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd27KzKnHlfQ"
      },
      "outputs": [],
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
        "        super().__init__()\n",
        "        # Initialize the modules we need to build the network\n",
        "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.act_fn = nn.Tanh()\n",
        "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Perform the calculation of the model to determine the prediction\n",
        "        x = self.linear1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf7kZo3uHlfQ"
      },
      "source": [
        "---\n",
        "## Part 13 (1 point)\n",
        "\n",
        "For the examples in this notebook, we will use a tiny neural network with two input neurons and four hidden neurons. As we perform binary classification, we will use a single output neuron. Note that we do not apply a sigmoid on the output yet. This is because other functions, especially the loss, are more efficient and precise to calculate on the original outputs instead of the sigmoid output.\n",
        "\n",
        "*   Use the `SimpleClassifier` class to build a NN called `model` with the appropriate number of inputs neurons, hidden neurons, and output neurons.\n",
        "*  Print your NN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICBUvMlaHlfR",
        "outputId": "d864077d-9298-433e-c439-41028079e174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleClassifier(\n",
            "  (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
            "  (act_fn): Tanh()\n",
            "  (linear2): Linear(in_features=4, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_FQPjMEHlfR"
      },
      "source": [
        "---\n",
        "## Part 14 (1 points)\n",
        "\n",
        "The parameters of a module can be obtained by using its `parameters()` function, or `named_parameters()` to get a name to each parameter object. Loop over all of the parameters and display them with their names.  You can loop over the named_parameters using a strategy such as:\n",
        "\n",
        "`for name, param in model.named_parameters():\n",
        "`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPoR-od0HlfR",
        "outputId": "f5a5c1ab-df6b-4916-ce53-82091abac4ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter linear1.weight, shape torch.Size([4, 2])\n",
            "Parameter linear1.bias, shape torch.Size([4])\n",
            "Parameter linear2.weight, shape torch.Size([1, 4])\n",
            "Parameter linear2.bias, shape torch.Size([1])\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR2voX7-HlfS"
      },
      "source": [
        "Each linear layer has a weight matrix of the shape `[output, input]`, and a bias of the shape `[output]`. The tanh activation function does not have any parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFU2p3eYHlfS"
      },
      "source": [
        "---\n",
        "## Part 15 (1 point)\n",
        "\n",
        "\n",
        "PyTorch also provides functionality for loading training and test data efficiently, summarized in the package `torch.utils.data`.  \n",
        "*   Import this package and call it `data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snbde6XcHlfS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWrTIePHHlfS"
      },
      "source": [
        "The data package defines two classes which are the standard interface for handling data in PyTorch: `data.Dataset`, and `data.DataLoader`. The Dataset class provides a uniform interface to access training/test data, while the DataLoader allows us to efficiently load and stack data points from a dataset into batches during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z10F_zLaHlfS"
      },
      "source": [
        "---\n",
        "## Part 16 (1 point)\n",
        "\n",
        "To define a dataset in PyTorch, we simply specify two functions: `__getitem__`, and `__len__`. The get-item function has to return the $i$-th data point in the dataset, while the len function returns the size of the dataset. For the XOR dataset, we can define the dataset class as follows. Familiarize yourself with this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKrkQICRHlfT"
      },
      "outputs": [],
      "source": [
        "class XORDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, size, std=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            size - Number of data points we want to generate\n",
        "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.std = std\n",
        "        self.generate_continuous_xor()\n",
        "\n",
        "    def generate_continuous_xor(self):\n",
        "        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1\n",
        "        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.\n",
        "        # If x=y, the label is 0.\n",
        "        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n",
        "        label = (data.sum(dim=1) == 1).to(torch.long)\n",
        "        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n",
        "        data += self.std * torch.randn(data.shape)\n",
        "\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the idx-th data point of the dataset\n",
        "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
        "        data_point = self.data[idx]\n",
        "        data_label = self.label[idx]\n",
        "        return data_point, data_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLJN5KnYHlfT"
      },
      "source": [
        "You should see from the above code that each data point contains two values and has an associated label.\n",
        "\n",
        "*   Use this class to create a dataset, called `dataset`, containing 200 data points.\n",
        "*   Use the `len` method to print the size of the dataset.\n",
        "*   Index into the dataset to print the first data point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tV9fzFKHlfT",
        "outputId": "09dc8d50-05bf-40a0-ff38-c62bf7efff9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of dataset: 200\n",
            "Data point 0: (tensor([1.0164, 0.9684]), tensor(0))\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI22o14bHlfU"
      },
      "source": [
        "---\n",
        "## Part 17 (1 point)\n",
        "\n",
        "We can create a function that allows us to visualize the samples. Familiarize yourself with this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qvz0VwvHlfU"
      },
      "outputs": [],
      "source": [
        "def visualize_samples(data, label):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.cpu().numpy()\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.cpu().numpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Use this function to plot all of the data points in your dataset."
      ],
      "metadata": {
        "id": "c2AtHwG5sLHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkEfwrNOHlfV",
        "outputId": "9d214930-50b5-4c0a-aa6d-a68f7fe4d7eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W9Vs_73HlfV"
      },
      "source": [
        "---\n",
        "## Part 18 (1 point)\n",
        "\n",
        "The class `torch.utils.data.DataLoader` represents a Python iterable over a dataset with support for automatic batching, multi-process data loading and many more features. The dataloader communicates with the dataset using the function `__getitem__`, and stacks its outputs as tensors over the first dimension to form a batch.\n",
        "In contrast to the dataset class, we usually don't have to define our own dataloader class, but can just create an object of it with the dataset as input.\n",
        "\n",
        "*   Create a dataloader object called `data_loader` using the options `batch_size=8` and `shuffle=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyPlTJ35HlfV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 19 (1 point)\n",
        "\n",
        "*   Get one batch of data from your dataset with your dataloader.  \n",
        "\n",
        "You can do this with `next(iter(data_loader))`.  This will return two outputs: a set of data and their associated labels (the variables specified in the `__getitem__` function in your dataset class).  Call these variables `data_inputs` and `data_labels`.\n",
        "*   Print `data_inputs` and `data_labels`."
      ],
      "metadata": {
        "id": "SjxkRWlkytwU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT2UAfZCHlfV",
        "outputId": "29086f74-9e8c-4138-ed5d-765879716d3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data inputs tensor([[-0.0568,  0.0720],\n",
            "        [-0.1278, -0.0888],\n",
            "        [ 0.0179,  1.0764],\n",
            "        [ 0.1385,  0.9818],\n",
            "        [ 0.0099,  1.0494],\n",
            "        [-0.0846,  1.0159],\n",
            "        [-0.0392,  0.0312],\n",
            "        [ 1.1843,  0.8750]])\n",
            "Data labels tensor([0, 0, 1, 1, 1, 1, 0, 0])\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvo7pCLsHlfW"
      },
      "source": [
        "---\n",
        "\n",
        "After defining the model and the dataset, it is time to prepare the optimization of the model. During training, we will perform the following steps:\n",
        "\n",
        "1. Get a batch of data from the dataloader\n",
        "2. Obtain the predictions from the model for the batch\n",
        "3. Calculate the loss based on the difference between predictions and true labels\n",
        "4. Backpropagation: calculate the gradients for every parameter with respect to the loss\n",
        "5. Update the parameters of the model in the direction of the gradients\n",
        "\n",
        "We have seen how we can do step 1, 2 and 4 in PyTorch. Now, we will look at step 3 and 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0rXTJKEHlfW"
      },
      "source": [
        "---\n",
        "## Part 20 (1 point)\n",
        "\n",
        "Next, we need to define a loss function for our classification problem.  Luckily, PyTorch can help us with this too! PyTorch provides a list of predefined loss functions which we can use (see [here](https://pytorch.org/docs/stable/nn.html#loss-functions) for a full list). Here, we'll use the module `nn.BCEWithLogitsLoss`.  The loss function applies a sigmoid to the output and calculates Binary Cross Entropy (BCE) loss, which is defined as follows:\n",
        "\n",
        "$$\\mathcal{L}_{BCE} = -\\sum_i \\left[ y_i \\log x_i + (1 - y_i) \\log (1 - x_i) \\right]$$\n",
        "\n",
        "where $y$ are our labels, and $x$ our predictions, both in the range of $[0,1]$.  In brief, this loss function calculates the similarity between the actual and predicted probabilities of points belonging to each class.\n",
        "*   Create a `BCEWithLogitsLoss` loss function and call it `loss_module`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alpMvHvOHlfW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBeKiWcMHlfX"
      },
      "source": [
        "---\n",
        "## Part 21 (1 point)\n",
        "\n",
        "For updating parameters, we can use the PyTorch package `torch.optim`, which has most popular optimizers implemented. We will use one of the most common: `torch.optim.SGD`. **Stochastic Gradient Descent** updates parameters by multiplying the gradients with a small constant, called learning rate, and subtracting those from the parameters (hence minimizing the loss). Therefore, we slowly move towards the direction of minimizing the loss. A good default value of the learning rate for a small network such as ours is 0.1.\n",
        "*   Create an optimizer object for SGD with a learning rate of 0.1.  Call this object `optimizer`.  Inputs to the optimizer are the parameters of the model (`model.parameters()`) and any optional settings.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsD7CZykHlfX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWcXaZcjHlfX"
      },
      "source": [
        "The optimizer provides two useful functions: `optimizer.step()` and `optimizer.zero_grad()`.\n",
        "\n",
        "The `.step()` function updates the parameters based on the gradients as explained above.\n",
        "\n",
        "The function `optimizer.zero_grad()` sets the gradients of all parameters to zero.\n",
        "\n",
        "While this function seems less relevant at first, it is a crucial pre-step before performing backpropagation. If we call the `backward` function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be added to the previous ones instead of overwriting them. This is done because a parameter might occur multiple times in a computation graph, and we need to sum the gradients in this case instead of replacing them. Hence, remember to call `optimizer.zero_grad()` before calculating the gradients of a batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCX6YZ1sHlfX"
      },
      "source": [
        "---\n",
        "## Part 22 (1 point)\n",
        "Finally, we are ready to train our model. As a first step, create a larger dataset (2500 data points) and specify a dataloader with a larger batch size (128). Call the dataset `train_dataset` and the dataloader `train_data_loader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pFc1myEHlfY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLQfl-f2HlfY"
      },
      "source": [
        "---\n",
        "## Part 23 (1 point)\n",
        "\n",
        "Now, we can write a small training function. Remember our five steps: load a batch, obtain the predictions, calculate the loss, backpropagate, and update. Additionally, we have to push all data and model parameters to the device of our choice (GPU if available). For the tiny neural network we have, communicating the data to the GPU actually takes much more time than we could save from running the operation on GPU. For large networks, the communication time is significantly smaller than the actual runtime making a GPU crucial in these cases. Still, to practice, we will push the data to GPU here.\n",
        "*   Push your network (`model`) to your GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOSf3kwVHlfY",
        "outputId": "dcfbadf0-b131-449c-f65f-07ea1c568da1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleClassifier(\n",
              "  (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
              "  (act_fn): Tanh()\n",
              "  (linear2): Linear(in_features=4, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# Push model to device. Has to be only done once\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZxb2UF_HlfY"
      },
      "source": [
        "---\n",
        "## Part 24 (5 points)\n",
        "\n",
        "Write a function, `train_model`, to train your model.  This function should be structured as follows:\n",
        "\n",
        "Inputs: Your model, your optimizer, the dataloader, the loss module, and the number of training epochs, which you can set to a default of 100 (i.e. `..., num_epochs=100):`.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1) Set your model to training mode. This is done by calling `model.train()`. There exist certain modules that need to perform a different forward step during training than during testing (e.g. BatchNorm and Dropout), and we can switch between them using `model.train()` and `model.eval()`.\n",
        "\n",
        "2) Write two nested `for` loops to iterate over the number of training epochs and the contents of the data_loader.  The syntax for the latter will be similar to what you wrote in **Part 19** but should use a `for` loop to iterate over all the training data rather than `next(iter(...))`, which grabs one set of data.  Inside the loops, do the following:\n",
        "\n",
        "3) Move `data_inputs` and `data_labels` to your device.\n",
        "\n",
        "4) Run the model on the input data.  Save the output as a variable, `preds`.  Remove singleton dimensions (dimensions of size 1), using a command such as `preds = preds.squeeze(dim=1)`.\n",
        "\n",
        "5) Use your loss module to calculate the loss according to `preds` and `data_labels`.  You might have to convert the latter to floats using `.float()` for use in your loss module.  Call the loss `loss`.\n",
        "\n",
        "6) Zero the gradient of your optimizer.\n",
        "\n",
        "7) Perform backpropogation on `loss` using `.backward()`.\n",
        "\n",
        "8) Update your parameters using the `.step()` method of your optimizer.\n",
        "\n",
        "*  Finally call `train_model` with the appropriate inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8Xlq0l3HlfZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgXrcxo9Hlfa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6fHcarfHlfb"
      },
      "source": [
        "---\n",
        "After finish training a model, we can extract the so-called `state_dict` from the model which contains all learnable parameters. For our simple model, the state dict contains the following entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5MDx9oZHlfb",
        "outputId": "207459b2-4396-4c12-afe4-87ee2d9b80ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear1.weight', tensor([[ 2.9523, -2.8716],\n",
            "        [-0.0966,  0.1669],\n",
            "        [-2.2212,  2.2694],\n",
            "        [-2.6768,  2.7202]])), ('linear1.bias', tensor([1.4015, 1.5003, 1.1470, 1.3607])), ('linear2.weight', tensor([[-4.6200,  2.0229, -2.0850, -3.0175]])), ('linear2.bias', tensor([2.4915]))])\n"
          ]
        }
      ],
      "source": [
        "state_dict = model.state_dict()\n",
        "print(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G89WqiXuHlfd"
      },
      "source": [
        "---\n",
        "## Part 26 (1 point)\n",
        "\n",
        "\n",
        "Once we have trained a model, it is time to evaluate it on a held-out test set. As our dataset consists of randomly generated data points, we can just create more data to use as a test set and create a corresponding dataloader.\n",
        "*   Just as you did before, create a dataset.  This time, create a dataset with 500 entries and call it `test_dataset`.\n",
        "*   Create a dataloader, called `test_data_loader` setting `batch_size=128`, `shuffle=False`, and `drop_last=False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAkvEKsAHlfd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWc-sswPHlfk"
      },
      "source": [
        "---\n",
        "## Part 27 (2 points)\n",
        "As metric, we will use accuracy which is calculated as follows:\n",
        "\n",
        "$$acc = \\frac{\\#\\text{correct predictions}}{\\#\\text{all predictions}} = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
        "\n",
        "where TP are the true positives, TN true negatives, FP false positives, and FN the fale negatives.\n",
        "\n",
        "When evaluating the model, we don't need to keep track of the computation graph as we don't intend to calculate the gradients. This reduces the required memory and speeds up the model. In PyTorch, we can deactivate the computation graph using `with torch.no_grad(): ...`. We also have to set the model to eval mode.\n",
        "*   Fill in code where indicated below to complete the model evaluation function\n",
        "*   Run the model evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVc-DErPHlfk"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "\n",
        "            # FILL IN CODE HERE:  Determine prediction of model on testing data.\n",
        "            # Send data_inputs and data_labels to your device\n",
        "            #vRun your model on the data_inputs and remove singleton dimensions\n",
        "\n",
        "\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # FILL IN CODE HERE: Update running totals of true positives and the number of predictions made\n",
        "\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "\n",
        "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1vHKYTXHlfl",
        "outputId": "002aa110-250e-4dc1-8399-8fb2a3bc1008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 100.00%\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4xID5dTHlfl"
      },
      "source": [
        "If we trained our model correctly, we should see a score close to 100% accuracy. However, this is only possible because of our simple task, and unfortunately, we usually don't get such high scores on test sets of more complex tasks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}